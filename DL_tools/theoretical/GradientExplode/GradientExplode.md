[TOC]

### 问题描述

[参考链接](https://blog.csdn.net/qq_25737169/article/details/78847691)，[参考链接2](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)

> 梯度爆炸问题的原因类似于梯度消失问题：深度神经网络中权值更新的规则导致每层梯度在反向传播的过程中逐层积累，如果使用的激活函数梯度小于1（sigmoid或者tanh）那么引发 梯度消失问题，越靠前的层梯度越小（越趋向于0），如果使用的激活函数大于1（exp、linear这类激活函数），则会使权重越来越大，最终甚至可能溢出变为Nan。
>

问题原因：

1）每层的梯度过大，且网络结构较深，逐层积累引发问题。

**梯度爆炸发生时一般伴随着反常、过大的（甚至会出现NaN）loss值以及梯度、权重值。**

每一层非线性层都可以视为是一个非线性函数 f(x)(非线性来自于非线性激活函数），因此整个深度网络可以视为是一个复合的非线性多元函数*: F*(*x*)=*fn*(...*f*3(*f*2(*f*1(*x*)∗*θ*1+*b*)∗*θ*2+*b*)...

### 几种潜在的解决方案

#### 改变激活函数

改变激活函数是可以最直观最有效的解决梯度爆炸问题的方法，一般来讲，深度学习中常用的Relu、Sigmoid、Tanh的梯度最大值都不超过1，可以有效地避免梯度爆炸问题。然而在使用relu作为新的激活函数是就需要面对dyingrelu的风险，tanh和sigmoid在网络层次较深时可能会触发梯度消失问题，这些都是需要一并重新解决的问题。

有效性分析：在实验中，改变激活函数是解决梯度爆炸问题最有效的方法之一，它从根本上改变了每层逐渐累积增长的梯度，使之小于等于1，避免爆炸。但是在特定网络结构中有可能不能完全解决问题，从而引发梯度消失或者dyingrelu等派生问题。

#### [使用BatchNormlization](https://blog.csdn.net/qq_25737169/article/details/79048516)

[参考链接2](https://www.cnblogs.com/guoyaohua/p/8724433.html)
[参考链接3](深度学习2017-271页)

BatchNormlization最早由谷歌小组的[paper](https://arxiv.org/pdf/1502.03167.pdf)提出，BN针对白化操作（就是对输入数据分布变换到0均值，单位方差的正态分布，但是代价很昂贵）作出了两种便捷化的改进：1）通过使每个标量特征的均值为零且方差为1来独立地对每个标量特征进行归一化；2）对每个batch都会产生每次激活的均值和方差的估计值。

BN的思想：**通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，使得梯度变大，易于训练**

论文中说到：如果可以确保网络训练时非线性输入的分布更稳定，则优化器不容易陷入饱和状态，更容易快速的训练。基于这种思想提出了BN：**BN通过固定图层输入的均值和方差的归一化步骤来完成操作；此外BN还可以通过减少梯度对参数或其初始值的大小的依赖性，对通过网络的梯度流也具有有益的影响**。 BN通过对整个网络的激活进行标准化，可以防止对参数的细微更改放大为梯度激活中的较大和次佳的更改，还使得训练对参数规模更具弹性。 

在深层次网络的更新中，每次更新的权重改变量会有显著的互相影响，BN通过重参数化，使得低层参数大多数情况下没什么影响，避免了每次更新对层输出统计量的极端影响。整体来看BN对于模型的训练是非常有益的。

有效性分析：对于每层反向传播中梯度逐渐变大最终导致爆炸的情况，BN可以通过归一化削弱爆炸现象，此外BN也可以有效的缓解梯度消失问题，具备较高的解决问题能力，绝大多数情况下是可以考虑的。然而，在实验中，如果使用exp激活函数，可能会出现一开始训练每层已经是溢出导致的NaN梯度了，这种情况下就无法进行归一化，该方法就会失效。*泛用性上不如改变激活函数*

#### [Gradient Clipping（clipnorm、clipvalue）](https://towardsdatascience.com/what-is-gradient-clipping-b8e815cdfb48)

梯度裁剪旨在通过将训练时反向传播的梯度控制在设定好的上下界中以避免训练时由于梯度过大而引发不稳定，相比于BN更加简单直观，对原本训练配置的修改很小，但是效果也更差，也无法很好地解决梯度消失问题。

有效性分析：对于每层反向传播中梯度逐渐变大最终导致爆炸的情况，clip可以通过削减梯度改善爆炸问题，但是效果收到一定的局限，尤其是clipvalue进行的直接裁剪，可能会影响到收敛的效率。

#### Resnet

[参考链接2：](https://my.oschina.net/u/876354/blog/1622896)

参考DyingRelu文档中提到的Resnet描述。

Resnet残差网络直接把输入x传到输出作为初始结果，输出结果为H(x)=F(x)+(-)x，当F(x)=0时，那么H(x)=x，也就是所谓的恒等映射。于是，ResNet相当于将学习目标改变了，不再是学习一个完整的输出，而是目标值H(X)和x的差值，也就是所谓的残差F(x) := H(x)-x，因此，后面的训练目标就是要将残差结果逼近于0，使到随着网络加深，准确率不下降。这使得Resnet在深层次网络结构中可以有非常强大的学习能力。

有效性分析：经典的残差网络Resnet采用relu激活函数，本身可以解决梯度爆炸问题，并有较好的学习效率。使用resnet网络架构解决问题可以避免梯度爆炸的同时，保证整体训练的质量。

#### LSTM

补充;LSTM一般不用来解决梯度爆炸问题，事实上如果使用relu激活函数，LSTM还可能发生梯度爆炸问题。但可以一定程度上改善梯度消失问题，可以[参考](https://www.zhihu.com/question/34878706)。



#### 总结

> 目前的潜在解决方案中，对网络修改程度由高到低：Resnet>BN>改变激活函数>gradientclip；目前的测试中，测试效果由高到低：改变激活函数>BN>gradientclip(Resnet缺乏实验)。
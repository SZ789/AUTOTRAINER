[TOC]



### 问题描述

> 训练不收敛问题的表现类似于梯度消失问题，但是又有所不同：梯度消失会存在一些靠前的层存在非常小的梯度（非常接近与0），而训练不收敛问题不会有这样的表现，一般来讲训练不收敛问题很可能是过小的学习率导致的，最终模型收敛至局部最优而非全局最优。
>

不收敛原因：

1）学习率过小难以跳出局部最优。


### 几个潜在解决方案

#### [改变学习率](https://machinelearningmastery.com/learning-rate-for-deep-learning-neural-networks/)

有效性分析：学习率的改变可以很大程度上影响整个模型的训练效果，；过大的学习率会导致不稳定的训练，最终可能获得次优的结果，最坏的情况会导致层的权重爆炸、数值溢出，过小的学习率可能会使得模型陷入局部最优解甚至永远不会收敛。在训练中如果发生了训练不收敛问题且可以确定不是梯度消失，则可以考虑增大梯度重新训练。**在解决训练不收敛问题时，具备较低的代价和较好的效果。**

#### 改变优化器

[参考链接1](https://arxiv.org/abs/1904.09237)

有效性分析：Adam和SGD等优化器在训练收敛问题上的效率一直存在广泛的争议，针对不同优化问题，SGD和Adam会表现出不同的效率，我们很难一概而论哪个更加优秀。当我们面对一个难以解决的训练不收敛问题时，我们可以考虑切换不同的优化器进行测试。

#### 总结

现有的解决方案较为单一，一般来讲对于不收敛问题我们可以考虑直接改变学习率进行测试，如果效果不好在考虑改变优化器。
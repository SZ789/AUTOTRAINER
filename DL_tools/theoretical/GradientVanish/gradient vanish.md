[TOC]

### 梯度消失问题描述

在机器学习中，使用基于梯度的学习方法和反向传播训练人工神经网络时会遇到消失的梯度问题。
在这种方法中，在每次训练迭代中，每个神经网络的权重都会收到与误差函数相对于当前权重的偏导数成比例的更新。如果激活函数的导数长期小于1，在层数较深的神经网络的最开始几层可能会出现梯度变为一个极小值，使得权重值更新速度过慢，最后导致神经网络的不更新。在最常使用的两种激活函数tanh和sigmoid函数中，由于他们的梯度绝对值始终是小于1且反向传播算法是通过链式规则计算梯度，从而导致前面层的权重值更新过慢以及神经网络的不更新。
[参考链接](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)

**gradient vanish问题出现时，权值会更新过慢，神经网络没有办法很好的收敛**

梯度消失的原因：

1） 神经网络层数过深

2） 激活函数的导数长期处在小于1的范围内

### 几个可能有效的解决方案


#### [RELU](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)

文章描述：
本文提出一种新的激活函数-**relu**。文章中没有使用ReLU解决梯度消失等问题，但是说明了神经元稀疏性应用在神经网络中的合理之处以及优势、应用前景。此外文章也提及DyingReLU的潜在问题（*硬饱和为0可能会阻止梯度反向传播*），但是在文章的实验中表明，短层（depth=3）的DNN中，ReLU会对训练有很大的帮助。

relu是一种整流线性激活函数，它的函数表达式relu(x)=max(x,0)，在反向传播过程中，因为他的导数在x大于0的情况下等于1时，传播过程中不会持续的缩小梯度或者放大梯度，所以不存在梯度消失问题。同时因为不存在梯度消失问题，所训练的神经网络可以更加复杂，通常得到的收敛结果要好于sigmoid和tanh激活函数。
	
实验结果：
在我们的实验中，大部分的梯度消失问题时，替换掉原来的激活函数tanh和sigmoid，一般都可以解决；但是随着层数的加深，relu函数也可能因为出现dying relu的问题，这时候需要我们利用其他的解决方案去解决dying relu问题。
	
有效性分析：
在层数不是非常深的梯度消失问题，利用relu激活函数代替tanh和sigmoid激活函数，通常会得到很好的结果，可以从根源上缓解梯度消失问题，使得梯度不再逐层锐减，是目前调研到的**梯度消失问题最简单而有效的解决方案，但是同时也要注意替换relu函数所导致的dying relu 问题。**


#### [Batch_normalization](https://en.wikipedia.org/wiki/Batch_normalization)

文章描述：
归一化处理-Batch_normalization，是白化操作的简化，论文(https://arxiv.org/abs/1502.03167)首次应用到神经网络的学习中，它是在原来的每个隐含层的训练过程中添加新的归一化层，使得训练的结果更加的稳定，以此来解决梯度消失以及梯度爆炸的问题。归一化层的主要思想是通过重新更改原来神经网络中每层的输入为标准化输入（**通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布，使得梯度变大，易于训练**），从而使得梯度变得便于后续的训练。

实验结果：
在我们的实验中，利用BN去解决梯度消失问题时，相比较原来的训练时长，加入归一化层后，虽然BN和RELU的收敛结果差不多，但是BN的**训练时长增加较多**，对于**预测值的实验**等特定情况中，BN效果并不理想，适用范围存在局限。

有效性分析：
通过实验，我们可以得出结论，BN在一定程序上是可以解决梯度消失问题的，但是具备较大的时间代价也会相当的程度上改变网络的结构，但是由于他是将每一层进行归一化处理，可能对于值预测问题中的某些特征就会被忽略掉，因此影响后续的预测问题。可以作为一个备选方案。

#### [residual network](https://arxiv.org/abs/1512.03385)

[参考链接2](https://blog.csdn.net/qq_32172681/article/details/100177636)

文章描述：
残差网络是一种通过添加恒等块，即他们利用将前面层的输出x，以及上一层的输出结果f(x),进行简单的加或者减作为新一层的输入值H(x)=f(x)+x,这样使得反向传播过程导致的梯度消失得以解决。在文章中提出了一种新的resnet网络作为残差网络的应用，在各种大赛中表现非常好.

实验结果：
在我们的实验中，我们也通过添加恒等块，对产生梯度消失的问题进行解决，在加入恒等块后，原来存在的梯度消失问题得以解决，同时**训练结果要好于前面两种解决方案**。

有效性分析：
Resnet是通过残差神经网络的残差块来解决梯度消失问题的，基本神经单元被设计成了H(x)=f(x)+x，那么此时求导就会得到
$$
\frac{\partial X_{L}}{\partial X_{l}}=\frac{\partial X_{l}+F\left(X_{l}, W_{l}, b_{l}\right)}{\partial X_{l}}=1+\frac{\partial F\left(X_{L}, W_{L}, b_{L}\right)}{\partial X_{L}}
$$
显然残差块的导数在传播中是大于1的，一定程度上避免了梯度消失问题。

​通过实验，我们进一步得出结论，残差网络可以较好的解决梯度消失问题，并且最后的训练结果也是好于其他方案的，但是Resnet网络结构几乎需要完全改变原有的网络的模型结构，并不是我们希望的自动修复方案，而更倾向于针对数据集的训练模型自动生成，因此这个方案可以作为各种方法无法解决梯度消失问题后的备选方案反馈给用户。

#### [Layer wise training](https://papers.nips.cc/paper/3048-greedy-layer-wise-training-of-deep-networks.pdf)

文章描述：
Layer wise Training是一个于2007年提出的一种用于解决梯度消失问题的方法。他的主要思想是，贪婪算法，每次仅仅训练一层，当该层训练完成后，在原来训练好的层权值保持不变的情况下，添加新的层进行训练，从而解决梯度消失问题。该方法主要有三方面：1）以贪婪的方式一次预训练一层； 2）在每一层使用无监督学习，以保留来自输入的信息；3）根据最终的关注标准调整整个网络。

实验结果：基于层的贪婪训练取得了相当好的结果，模型可以在相对较深的层次中仍保持很好地训练效果，然而所花费的时间也随着层数的增加同样的提升。
	
有效性分析：
该训练方法旨在对每层进行单独训练，使之短时间内具备较优的参数配置，最后在进行全局调整，获取最佳参数配置。通过每次只训练新加入的一层，相对有效地避免了层数过多而带来的传播中梯度消失问题，实验中证明其非常有效。然而使用该解决方案需要控制训练过程，且实现较为复杂，并且模型层数越深训练中花费时间越多。与前面三种方法不同，这种解决方案可以作为**备选方案**反馈给用户，一般不建议考虑。

#### [unsupervised pre-training](http://www.jmlr.org/papers/volume11/erhan10a/erhan10a.pdf)

[参考文献2：](http://proceedings.mlr.press/v15/glorot11a/glorot11a.pdf)

实验结果：实验中，基于无监督学习的预处理可以有效地改善较深层网络的梯度消失问题，7层的sigmoid全连接层既然可以有0.8以上的acc，有效的缓解了问题

有效性分析：通过无监督的预训练方法，将参数调整至最佳收敛区域，再进行后续训练辅助收敛。文章指出，无监督的预训练起到了正则化的作用，在具体之前就将参数限制在了一定的方位内进行微调，进而使得模型可以更快的收敛。基于无监督的预训练类似于前文提到的layerwise（基于监督学习的预训练），本质上都是通过预训练将参数限制在较好的区域内，在通过单层训练得到较优的参数组合，最后统一训练进行微调。

#### [Long short-term memory](https://www.bioinf.jku.at/publications/older/2604.pdf)

[参考链接2](https://www.zhihu.com/question/44895610)

[参考链接3](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

理论上RNN有学习长期依赖关系的能力，然而实际训练中表现很差，而LSTM可以很好地解决这个问题。RNN的重复模块中一般只包含一个单层，而LSTM包含四个交互层，三个门（遗忘门、输入门、输出门）用于保护和控制单元状态。LSTM工作时第一步确定从单元状态中丢弃的信息，第二步决定在单元状态下存储哪些新的信息，第三步更新单元格状态，第四步决定要输出什么。

三个门用如下函数控制流入流出：
$$
\begin{array}{l}
f_{t}=\sigma\left(W_{f} X_{t}+b_{f}\right) \\
i_{t}=\sigma\left(W_{i} X_{t}+b_{i}\right) \\
o_{i}=\sigma\left(W_{o} X_{t}+b_{o}\right)
\end{array}
$$
当前状态加上激活函数可以表示为：
$$
C_{t}=\sigma\left(W_{f} X_{t}+b_{f}\right) C_{t-1}+\sigma\left(W_{i} X_{t}+b_{i}\right) X_{t}\\
\frac{\partial C_{j}}{\partial C_{j-1}}=\sigma\left(W^{f} X_{j}+b\right)
$$
在实际参数更新中，可以通过控制bias比较大，使得该值接近于1；在这种情况下，即使通过很多次连乘的操作，梯度也不会消失，仍然可以保留&#34;长距&#34;连乘项的存在。即总可以通过选择合适的参数，在不发生梯度爆炸的情况下，找到合理的梯度方向来更新参数，而且这个方向可以充分地考虑远距离的隐含层信息的传播影响

文章描述：

​	lstm是一种用于解决RNN训练过程中的梯度消失问题，以及梯度爆炸问题。他的主要思想是，利用两个参数C<sub>t</sub>(cell state)和h<sub>t</sub>记录中间的状态,其中C<sub>t</sub>的状态一般变化不大，而h<sub>t</sub>由于输入的不同可能产生很大的不同，用来解决一些传统RNN网络的梯度消失的问题。

实验结果：
实验结果表明，LSTM依然可以发生梯度消失问题，但是总体表现要远好于普通的RNN，除非极大加深网络深度，否则在LSTM层很难触发梯度消失问题。

有效性分析：
通过实验分析可得，可以利用**LSTM解决RNN梯度消失问题**，但是由于要改变网络结构，手段较为复杂，因此可以作为RNN网络的一个**备用选项**。

#### 总结：

目前调研到可行性较高的三种方法中，通过实验可以得出，解决方案对原网络的修改程度由高到低- Residual Network>BN>Relu; 方案的有效性由高到底 Residual Network > Relu >BN。结合目前的实验结果，我们对梯度消失问题的采取的解决方案顺序是RELU，BN,Residual Network，。

Layer wise training和unsupervised pre-training可以作为备用选项解决方案。对于RNN的梯度消失问题可以用lstm解决。